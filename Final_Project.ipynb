{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_Project.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOw7oIWYpEUg7c6KqXn9wHO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Final Project\n","\n","## Author: Cho Laam Yuen"],"metadata":{"id":"I0X4E6xyE_nt"}},{"cell_type":"markdown","source":["## Load Data and Packages"],"metadata":{"id":"exGxsSnJE4bo"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3trVae1SEXv5","executionInfo":{"status":"ok","timestamp":1652193829232,"user_tz":240,"elapsed":34012,"user":{"displayName":"Cho Laam Yuen","userId":"10167076076156756338"}},"outputId":"7cdbe0d9-dca1-4196-c400-868fe0e0ada8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#connect to my google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["#change working directory\n","%cd /content/drive/My\\ Drive/Adv\\ ML/gr5074-final-project-ChoLaamY/\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iXkjKXpAGVe3","executionInfo":{"status":"ok","timestamp":1652193834397,"user_tz":240,"elapsed":266,"user":{"displayName":"Cho Laam Yuen","userId":"10167076076156756338"}},"outputId":"7bf424e9-dfbb-40c3-c0b0-facbfb57c7c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Adv ML/gr5074-final-project-ChoLaamY\n","/content/drive/My Drive/Adv ML/gr5074-final-project-ChoLaamY\n"]}]},{"cell_type":"code","source":["#load my libraries\n","import sys\n","import time\n","import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import tensorflow as tf\n","import os\n","import zipfile\n","\n","from skimage.transform import resize\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow.python.keras.utils import np_utils\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n","from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D \n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n","from tensorflow.keras.applications import VGG19, ResNet50, InceptionV3"],"metadata":{"id":"jNt01hQjFVhc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#code taken from example code for Covid_Image_Classification (HW2)\n","#extracting all filenames iteratively\n","base_path = 'thumbnails3'\n","categories = ['Entertainment', 'Informative', 'Tech',]\n","\n","# load file names to fnames list object\n","fnames = []\n","for category in categories:\n","    image_folder = os.path.join(base_path, category)\n","    file_names = os.listdir(image_folder)\n","    full_path = [os.path.join(image_folder, file_name) for file_name in file_names]\n","    fnames.append(full_path)\n","\n","print('number of images for each category:', [len(f) for f in fnames])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u3MbgkCyFbMz","executionInfo":{"status":"ok","timestamp":1652197405054,"user_tz":240,"elapsed":256,"user":{"displayName":"Cho Laam Yuen","userId":"10167076076156756338"}},"outputId":"a2ade406-ff46-4d59-e699-3ee927a53695"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["number of images for each category: [813, 716, 774]\n"]}]},{"cell_type":"markdown","source":["## Preprocess Data and Setup"],"metadata":{"id":"xzVg2eMLHFbD"}},{"cell_type":"code","source":["#writing the preprocessor\n","# Import image, load to array of shape height, width, channels, then min/max transform.\n","# Write preprocessor that will match up with model's expected input shape.\n","# Uses opencv for image preprocessing\n","\n","def preprocessor(data, shape=(320, 180)):\n","        \"\"\"\n","        This function reads in images, resizes them to a fixed shape, and\n","        min/max transforms them, before converting feature values to float32\n","        for ONNX.\n","        \n","        params:\n","            data\n","                list of unprocessed images\n","                      \n","        returns:\n","            X\n","                numpy array of preprocessed image data\n","                  \n","        \"\"\"\n","           \n","        import cv2\n","        import numpy as np\n","\n","        \"Resize a color image and min/max transform the image\"\n","        img = cv2.imread(data) # Read in image from filepath.\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # cv2 reads in images in order of blue green and red, we reverse the order for ML.\n","        #grayscale image?  Use im_gray = cv2.imread('gray_image.png', cv2.IMREAD_GRAYSCALE)\n","        img = cv2.resize(img, shape) # Change height and width of image.\n","        img = img / 255.0 # Min-max transform.  \n","\n","        # Resize the images.\n","        X = np.array(img)\n","        #X = np.expand_dims(X, axis=0) # Expand dims to add \"1\" to object shape [1, h, w, channels] if needed.\n","        X = np.array(X, dtype=np.float32) # Final shape for onnx runtime.\n","        return X\n"],"metadata":{"id":"l3VFGKQpHJPm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check whether the preprocessor worked\n","#(Height, Width, Channels)\n","preprocessor('thumbnails3/Tech/_O_kWL-YhZE.jpg').shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3kRnZxRMMRhO","executionInfo":{"status":"ok","timestamp":1652194161996,"user_tz":240,"elapsed":620,"user":{"displayName":"Cho Laam Yuen","userId":"10167076076156756338"}},"outputId":"74bae6e2-1b81-4fd8-f1d7-79c0f001ee7d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(180, 320, 3)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["#Import image files iteratively and preprocess them into array of correctly structured data\n","\n","# Create list of file paths\n","image_filepaths=fnames[0]+fnames[1]+fnames[2]\n","\n","# Iteratively import and preprocess data using map function\n","\n","# map functions apply your preprocessor function one step at a time to each filepath\n","preprocessed_image_data=list(map(preprocessor,image_filepaths ))\n","\n","# Object needs to be an array rather than a list for Keras (map returns to list object)\n","X= np.array(preprocessed_image_data) # Assigning to X to highlight that this represents feature input data for our model"],"metadata":{"id":"HoPmPCCMNJR8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#check length of files matched number of images in my dataset\n","len(image_filepaths)\n","print(len(X) ) #same number of images in dataset\n","print(X.shape ) #dimensions should be 180, 320, 3 for all images\n","print(X.min() ) #min value of every image is 0\n","print(X.max() ) #max value of every image is 1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFDnvd-vP9SR","executionInfo":{"status":"ok","timestamp":1652194942099,"user_tz":240,"elapsed":313,"user":{"displayName":"Cho Laam Yuen","userId":"10167076076156756338"}},"outputId":"001ce0b1-ff6e-43dd-f3f7-4afd9e7c30a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2303\n","(2303, 180, 320, 3)\n","0.0\n","1.0\n"]}]},{"cell_type":"code","source":["# Create y data made up of correctly ordered labels from file folders\n","from itertools import repeat\n","\n","# 3 folders with the corresponding number of images in each folder\n","\n","print('number of images for each category:', [len(f) for f in fnames])\n","Entertainment=list(repeat('Entertainment', 813))\n","Informative=list(repeat('Informative', 716))\n","Tech=list(repeat('Tech', 774))\n","\n","\n","#combine into single list of y labels\n","y_labels = Entertainment+Informative+Tech\n","\n","#check length, same as X above\n","print(len(y_labels) )\n","\n","# Need to one hot encode for Keras.  Let's use Pandas\n","\n","import pandas as pd\n","y=pd.get_dummies(y_labels)\n","\n","display(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":459},"id":"AWLoBvfAQEy4","executionInfo":{"status":"ok","timestamp":1652195047240,"user_tz":240,"elapsed":271,"user":{"displayName":"Cho Laam Yuen","userId":"10167076076156756338"}},"outputId":"15b97456-0154-4f44-a65c-fae988ec7229"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of images for each category: [813, 716, 774]\n","2303\n"]},{"output_type":"display_data","data":{"text/plain":["      Entertainment  Informative  Tech\n","0                 1            0     0\n","1                 1            0     0\n","2                 1            0     0\n","3                 1            0     0\n","4                 1            0     0\n","...             ...          ...   ...\n","2298              0            0     1\n","2299              0            0     1\n","2300              0            0     1\n","2301              0            0     1\n","2302              0            0     1\n","\n","[2303 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-cef88b6c-2411-48a4-a9ff-d5b0dfdfe873\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Entertainment</th>\n","      <th>Informative</th>\n","      <th>Tech</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2298</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2299</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2300</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2301</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2302</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2303 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cef88b6c-2411-48a4-a9ff-d5b0dfdfe873')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cef88b6c-2411-48a4-a9ff-d5b0dfdfe873 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cef88b6c-2411-48a4-a9ff-d5b0dfdfe873');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["#train test split the dataset\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.20, random_state = 874920)\n","\n","#check y_test set\n","y_test.sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AHT5AZIMTA7B","executionInfo":{"status":"ok","timestamp":1652195055497,"user_tz":240,"elapsed":595,"user":{"displayName":"Cho Laam Yuen","userId":"10167076076156756338"}},"outputId":"9dd763cb-0fc7-4798-941c-5e11fd807685"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Entertainment    163\n","Informative      143\n","Tech             155\n","dtype: int64"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## Visualizing the Thumbnails"],"metadata":{"id":"QeL02dMwTWkp"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from mpl_toolkits.axes_grid1 import ImageGrid\n","import numpy as np\n","import random\n","\n","pic1 = preprocessor('thumbnails3/Entertainment/_qIRtFE6aIc.jpg')\n","pic2 = preprocessor('thumbnails3/Entertainment/8GxqvnQyaxs.jpg')\n","pic3 = preprocessor('thumbnails3/Entertainment/n4bucphC9r4.jpg')\n","pic4 = preprocessor('thumbnails3/Informative/UCA1A5GqCdQ.jpg')\n","pic5 = preprocessor('thumbnails3/Informative/el6No1wNKf0.jpg')\n","pic6 = preprocessor('thumbnails3/Informative/tJevBNQsKtU.jpg')\n","pic7 = preprocessor('thumbnails3/Tech/3dEfc9LL9bQ.jpg')\n","pic8 = preprocessor('thumbnails3/Tech/bCu0Z71QRF0.jpg')\n","pic9 = preprocessor('thumbnails3/Tech/DTBu4tigSDo.jpg')\n","\n","#make a grid for the images\n","figure = plt.figure(figsize=(100, 100))\n","grid = ImageGrid(figure, 111,\n","                 nrows_ncols=(3, 3),\n","                 axes_pad=2,\n","                 )\n","\n","for ax, pic in zip(grid, [pic1, pic2, pic3, pic4, pic5, pic6, pic7, pic8, pic9]):\n","  ax.imshow(pic)\n","  \n","#adjust the text size  \n","  plt.rc('font', size=60)\n","\n","#create labels for images\n","  figure.text(0.13, 0.735, 'Entertainment', color=\"black\")\n","  figure.text(0.39, 0.735, 'Entertainment', color=\"black\")\n","  figure.text(0.66, 0.735, 'Entertainment', color=\"black\")\n","  figure.text(0.13, 0.58, 'Informative', color=\"black\")\n","  figure.text(0.39, 0.58, 'Informative', color='black')\n","  figure.text(0.66, 0.58, 'Informative', color='black')\n","  figure.text(0.13, 0.42, 'Tech', color='black')\n","  figure.text(0.39, 0.42, 'Tech', color='black')\n","  figure.text(0.66, 0.42, 'Tech', color='black')"],"metadata":{"id":"nn7kBuLNg6y-","colab":{"base_uri":"https://localhost:8080/","height":808,"output_embedded_package_id":"1YRODQjYbwZTBTgNpI51teuBn62HF84WB"},"executionInfo":{"status":"ok","timestamp":1652197483413,"user_tz":240,"elapsed":12606,"user":{"displayName":"Cho Laam Yuen","userId":"10167076076156756338"}},"outputId":"833da559-ec14-4b93-fd53-2cb60f644cc1"},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["## Model Experimentation"],"metadata":{"id":"_vOkvQYCYHiG"}},{"cell_type":"markdown","source":["#### Model 1"],"metadata":{"id":"CIFL6POXYTDa"}},{"cell_type":"code","source":["with tf.device('/device:GPU:0'):\n","\n","  model1 = Sequential()\n","  model1.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(180, 320, 3)))\n","  model1.add(Conv2D(filters=32, kernel_size=1, padding='same', activation='relu'))\n","  model1.add(MaxPooling2D(pool_size=2))\n","  model1.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n","  model1.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n","  model1.add(MaxPooling2D(pool_size=2))\n","  model1.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","  model1.add(Conv2D(filters=32, kernel_size=1, padding='same', activation='relu'))\n","  model1.add(MaxPooling2D(pool_size=2))\n","  model1.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","  model1.add(Conv2D(filters=32, kernel_size=1, padding='same', activation='relu'))\n","  model1.add(MaxPooling2D(pool_size=2))\n","\n","  model1.add(Dropout(0.1))\n","  model1.add(Flatten())\n","  model1.add(Dense(16, activation='relu')) # One fully-connected layer\n","  model1.add(Dropout(0.1))\n","\n","  model1.add(Dense(3, activation='softmax'))\n","\n","  model1.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n","\n","model1.fit(X_train, y_train, \n","                    epochs = 5, verbose=1, validation_data=(X_test,y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQjX8mslYKxm","outputId":"9bea8aec-5235-48a0-d7ce-e75a3858b2c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","58/58 [==============================] - 126s 2s/step - loss: 1.0949 - accuracy: 0.3730 - val_loss: 1.0876 - val_accuracy: 0.4403\n","Epoch 2/5\n","58/58 [==============================] - 125s 2s/step - loss: 1.0637 - accuracy: 0.4490 - val_loss: 1.0554 - val_accuracy: 0.4338\n","Epoch 3/5\n","58/58 [==============================] - 130s 2s/step - loss: 1.0244 - accuracy: 0.4810 - val_loss: 1.0661 - val_accuracy: 0.4642\n","Epoch 4/5\n","58/58 [==============================] - 132s 2s/step - loss: 0.9927 - accuracy: 0.5098 - val_loss: 0.9398 - val_accuracy: 0.5835\n","Epoch 5/5\n","58/58 [==============================] - 130s 2s/step - loss: 0.9463 - accuracy: 0.5353 - val_loss: 1.0015 - val_accuracy: 0.5315\n"]}]},{"cell_type":"markdown","source":["#### Model 2 (VGG16)"],"metadata":{"id":"HR5-NTijqFZ3"}},{"cell_type":"code","source":["from tensorflow.python.keras import layers\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.python.keras.callbacks import ModelCheckpoint\n","with tf.device('/device:GPU:0'):\n","\n","  base_model = VGG16(input_shape=(180, 320, 3),\n","                   include_top=False,\n","                   weights='imagenet')\n","#freeze weights\n","base_model.trainable=False\n","\n","#flatten, add dense layer, dropout, and another dense layer \n","flat = Flatten()(base_model.layers[-1].output)\n","layer1 = Dense(16, activation='relu')(flat)\n","drop = Dropout(0.1)(layer1)\n","output = Dense(3, activation='softmax')(drop)\n","\n","model6 = Model(inputs=base_model.inputs, outputs=output)\n","\n","#different evaluation metrics\n","mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n","red_lr= ReduceLROnPlateau(monitor='val_acc',patience=2,verbose=1,factor=0.5, min_lr=0.001) # dividing lr by 2 when val_accuracy fails to improve after 2 epochs\n","\n","model6.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","model6.fit(X_train, y_train, \n","           epochs=5, verbose=1, validation_data=(X_test, y_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQOSEH1QqEte","outputId":"6f63b5c5-fbad-4953-f6a6-681bc7624886"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 0s 0us/step\n","58900480/58889256 [==============================] - 0s 0us/step\n","Epoch 1/5\n","58/58 [==============================] - 973s 17s/step - loss: 1.1373 - accuracy: 0.3936 - val_loss: 1.0179 - val_accuracy: 0.5011\n","Epoch 2/5\n","58/58 [==============================] - 969s 17s/step - loss: 0.9985 - accuracy: 0.5141 - val_loss: 0.9825 - val_accuracy: 0.5553\n","Epoch 3/5\n","58/58 [==============================] - 966s 17s/step - loss: 0.9137 - accuracy: 0.5489 - val_loss: 0.9820 - val_accuracy: 0.5228\n","Epoch 4/5\n","19/58 [========>.....................] - ETA: 8:43 - loss: 0.8834 - accuracy: 0.5444"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"Chq4HU6y2aqI"},"execution_count":null,"outputs":[]}]}